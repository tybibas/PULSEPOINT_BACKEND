import modal
from modal import App, Image, Secret, web_endpoint
from fastapi import Request
from fastapi.responses import JSONResponse
import os
import json
import datetime
from typing import List, Dict, Any

# Define the image with necessary dependencies
image = (
    Image.debian_slim()
    .pip_install(
        "apify-client",
        "openai",
        "supabase",
        "python-dotenv",
        "fastapi"
    )
)

app = App("pulsepoint-sourcing-engine")

# ==================== HELPERS (Adapted from source_new_accounts.py) ====================

def get_db():
    from supabase import create_client
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
    if not url or not key:
        raise ValueError("Missing Supabase credentials")
    return create_client(url, key)

def get_ai():
    from openai import OpenAI
    return OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

def get_apify():
    from apify_client import ApifyClient
    return ApifyClient(os.environ.get("APIFY_API_KEY"))

def build_advanced_queries(criteria: dict) -> List[str]:
    """Construct high-precision Google queries."""
    industries = criteria.get("icp_industries", [])
    keywords = criteria.get("icp_keywords", [])
    location = criteria.get("icp_location", "United States")
    
    queries = []
    
    # Base Terms
    base_terms = " OR ".join([f'"{k}"' for k in keywords[:3]]) if keywords else ""
    ind_terms = " OR ".join([f'"{i}"' for i in industries[:3]]) if industries else ""
    
    # 1. "Best X in Y" (Capture Directory/Listicle overlap)
    if industries and location:
        queries.append(f'top {industries[0]} companies in "{location}" -jobs -careers')
        
    # 2. Competitor/Related (if keywords provided)
    if base_terms:
        queries.append(f'{base_terms} "{location}" ("about us" OR "our team") -site:linkedin.com')
        
    # 3. News Trigger (Recent funding, growth)
    if ind_terms:
         queries.append(f'{ind_terms} ("series a" OR "raised" OR "growth" OR "hiring") "{location}" site:techcrunch.com OR site:businesswire.com OR site:prnewswire.com')
        
    return queries

def search_google(queries: List[str], max_results: int = 50) -> List[dict]:
    """Run Deep Google Search via Apify."""
    client = get_apify()
    all_results = []
    seen_urls = set()
    
    print(f"ðŸ” Running {len(queries)} Google Searches...")
    
    for q in queries:
        try:
            run_input = {
                "queries": q,
                "resultsPerPage": 20, 
                "maxPagesPerQuery": 2,
                "countryCode": "us",
            }
            # Start the actor
            run = client.actor("apify/google-search-scraper").call(run_input=run_input)
            
            # Fetch results
            dataset = client.dataset(run["defaultDatasetId"])
            for item in dataset.iterate_items():
                for res in item.get("organicResults", []):
                    url = res.get("url")
                    if url and url not in seen_urls:
                        # Basic Cleanup filters
                        if "linkedin.com" in url or "glassdoor.com" in url or "indeed.com" in url:
                            continue
                            
                        seen_urls.add(url)
                        all_results.append(res)
                        
            if len(all_results) >= max_results:
                break
                
        except Exception as e:
            print(f"âš ï¸ Search failed for '{q}': {e}")
            
    return all_results[:max_results]

def ai_gatekeeper_check(company_data: dict, criteria: dict) -> tuple:
    """The 'Deep Specificity' Engine."""
    client = get_ai()
    
    constraints = criteria.get("icp_constraints", [])
    negatives = criteria.get("icp_negative_keywords", [])
    
    prompt = f"""
    You are a Strict Investment Analyst. 
    Analyze this company based on the provided search result.
    
    SEARCH RESULT:
    Title: {company_data.get('title')}
    Desc: {company_data.get('description')}
    URL: {company_data.get('url')}
    
    ICP REQUIREMENTS:
    - Must match Industries: {criteria.get('icp_industries')}
    - Location: {criteria.get('icp_location')}
    
    STRICT CONSTRAINTS (Fail if violated):
    {json.dumps(constraints, indent=2)}
    
    NEGATIVE KEYWORDS (Fail if present):
    {json.dumps(negatives, indent=2)}
    
    TASK:
    1. Extract Company Name (clean version).
    2. Determine if they are a VALID FIT for a service business outreach.
    3. If there is insufficient info to disqualify, lean towards "Tentative Pass".
    4. If they violate a constraint, REJECT immediately.
    
    Return JSON:
    {{
        "company_name": "Name",
        "is_fit": true/false,
        "reason": "Why fit or why rejected"
    }}
    """
    
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        content = json.loads(resp.choices[0].message.content)
        return (content.get("is_fit", False), content.get("reason", "Unknown"), content)
    except Exception as e:
        return (False, f"AI Error: {e}", {})

def enrich_and_insert(supabase, valid_companies: List[dict], client_context: str, strategy_id: str) -> int:
    """Insert qualified leads into Supabase."""
    count = 0
    for comp in valid_companies:
        name = comp.get("company_name", "Unknown")
        print(f"   ðŸ’¾ Processing: {name}")
        
        # 1. Deduplication
        clean_name = name.lower().replace("inc", "").replace("llc", "").strip()
        # Only check against this client's leads if possible, but global check is safer for now
        existing = supabase.table("triggered_companies").select("id").ilike("company", f"%{clean_name}%").execute()
        
        if existing.data:
            print(f"      Duplicate detected. Skipping.")
            continue
            
        # 2. Insert
        data = {
            "company": name,
            "client_context": client_context,
            "strategy_id": strategy_id,
            "website": comp.get("url"),
            "monitoring_status": "active",
            "last_monitored_at": "2000-01-01 00:00:00", # Force immediate scan
            "events_history": [{"source": "auto_sourced", "date": datetime.datetime.now().isoformat()}]
        }
        
        try:
            supabase.table("triggered_companies").insert(data).execute()
            print(f"      âœ… Inserted {name}")
            count += 1
        except Exception as e:
            print(f"      âŒ Insert Failed: {e}")
            
    return count

# ==================== MODAL FUNCTION ====================

@app.function(
    image=image,
    secrets=[Secret.from_dotenv()],
    timeout=900 # 15 minutes max
)
def run_sourcing_job(strategy_id: str, criteria: dict):
    print(f"ðŸš€ [Remote] Starting Sourcing Job for Strategy: {strategy_id}")
    
    db = get_db()
    
    # verify strategy exists and get context slug
    strat_resp = db.table("client_strategies").select("slug").eq("id", strategy_id).execute()
    if not strat_resp.data:
        print("âŒ Strategy not found in DB.")
        return {"success": False, "message": "Strategy not found"}
        
    client_context = strat_resp.data[0].get("slug")
    
    # 1. Build Queries
    queries = build_advanced_queries(criteria)
    target_count = criteria.get("target_count", 10)
    
    if not queries:
        return {"success": False, "message": "No valid queries generatable from criteria"}

    # 2. Search
    # Overshoot logic: Fetch 3x to allow for 66% filtration rate
    raw_results = search_google(queries, max_results=target_count * 3)
    print(f"ðŸ“¬ Retrieved {len(raw_results)} Raw Candidates. Gatekeeping...")
    
    # 3. AI Gatekeeper
    valid_companies = []
    for res in raw_results:
        if len(valid_companies) >= target_count:
            break
            
        is_fit, reason, data = ai_gatekeeper_check(res, criteria)
        
        if is_fit:
            print(f"   âœ… PASS: {data.get('company_name')}")
            if not data.get("url"): data["url"] = res.get("url")
            valid_companies.append(data)
            
    print(f"ðŸ’Ž Validated {len(valid_companies)} Fits.")
    
    # 4. Insert
    inserted_count = enrich_and_insert(db, valid_companies, client_context, strategy_id)
    
    return {
        "success": True, 
        "leads_added": inserted_count, 
        "message": f"Successfully sourced {inserted_count} new accounts."
    }

# ==================== WEB ENDPOINT ====================

@app.function(image=image)
@web_endpoint(method="POST")
def trigger_sourcing(item: Dict[str, Any]):
    """
    Webhook called by Supabase Edge Function.
    Payload: { "strategy_id": "...", "criteria": {...} }
    """
    strategy_id = item.get("strategy_id")
    criteria = item.get("criteria")
    
    if not strategy_id or not criteria:
        return JSONResponse(status_code=400, content={"error": "Missing inputs"})
        
    print(f"ðŸ”” Received Trigger for Strategy {strategy_id}")
    
    # Option A: Blocking call (wait for results) - Good for immediate feedback < 60s
    # Option B: Spawn background job - Good for long tasks > 60s
    
    # Sourcing can take 2-3 minutes. We must SPAWN it.
    # However, the frontend expects a result count.
    # If we spawn, we return 0 immediately.
    # Given the user wants to see "leads added", we might try blocking, but Modal Web Endpoints timeout at 60s.
    # Sourcing 50 leads WILL take > 60s.
    
    # CORRECT PATTERN: Spawn the job, return "Job Started", and let the job update the DB.
    # The frontend will just have to trust it started.
    
    # BUT: The current Edge Function waits for a response with "leads_added".
    # Let's try blocking call first. If it times out, we move to background.
    # Actually, for 10-50 leads, it's safer to block the Edge Function, but the Webhook itself?
    
    # Let's use `.remote()` which blocks until completion. 
    # NOTE: Set the Edge Function timeout to logic? Supabase EFs timeout at 40s (wall clock) usually?? 
    # Actually, Supabase Edge Functions standard timeout is surprisingly short.
    # Standard is 10s?? No, can be more.
    
    # REALITY CHECK: We cannot wait 3 minutes for 50 leads in a synchronous HTTP call.
    # We must treat this as "Job Started".
    # I will spawn the job.
    
    # Re-reading user request: "start populating them in the right places"
    # The user accepts that they populate.
    # I will verify if I can return a "Job Started" message.
    
    try:
        # We will use .spawn() to run in background.
        # But for 'immediate gratification' tests with count=10, maybe we block?
        # Let's block. If it fails, we advise async.
        result = run_sourcing_job.remote(strategy_id, criteria)
        return JSONResponse(content=result)
    except Exception as e:
        print(f"Error: {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})
